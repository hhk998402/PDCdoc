## 4. Requirement Specifications:

### 4.1 Hardware Requirement

We require a computer system which has minimum 8 GB RAM and 8 GB disk space to run the virtual machine. The system should support 64-bit.

### 4.2 Software Requirement

We have used Oracle Virtualbox as the virtualization hypervisor on which we ran the Cloudera (python) virtual machine (cloudera-quickstart-vm-5.4.2-0-virtualbox). This gives as the necessary environment required to run the Hadoop ecosystem. It uses CentOS. 

### 4.3 Language Used

The mapper and reducer files have been coded in Python language. Basic knowledge of the Hadoop ecosystem is required to implement this project.

### 4.4 Data Set Used 
The input file that we have chosen is that of a product inventory of a chain store located in many places in the United States. <br>
The dataset, purchases.txt contains six columns separated by tab spaces.

![purchases.txt](https://github.com/Sid2110/PDC_project/blob/master/PaymentAnalysis/hadoop_purchase.JPG)

## System Design

1. **MapReduce is a type of Divide and Conquer Programming model.**

MapReduce algorithm works on dividing the data into many blocks, and processing them individually and recombining them for getting the result. 

a) HBase uses BloomFilters for Reading. <br>
b) Hive uses Avro, RC or ORC type of storage formats. <br>

2. **The output of the mapper is stored as a SequenceFile format.** <br>
-- Quite similar to the Collections object in Java

### Implementation Details:

For this project we be aimed to complete the following objectives:-
	
1. Arrange a big data file. Since we won't be able to store a big data file on our conventional storage devices so we'll have it in a compressed form.
2. We'll then interpret the data using the software named *HADOOP*.  <br>
Basically there's six tasks involved in data mining: <br>
  a) Anomaly detection <br>
  b) Association of the detected data/rule learning <br>
  c) Clustering <br>
  d) Cluster classification <br>
  e) Regression <br>
  f) Summarization <br>
3. After we mined the data, the results were collected.
4. As soon as the results are validated the data was graphed.

## 6. Modules 

### 6.1 Installation and Initialisation of Software

The software have been downloaded from the respective websites of VirtualBox and Cloudera. The setup for the cloudera VM can be followed at this link [Setting up Cloudera VM](https://www.youtube.com/watch?v=L0lPPC5qeyU). 

We have set the environment factors like the disk space, RAM, number of CPUs, etc as 8 GB, 8GB, 1 CPU respectively. 

After installation, all the required system files are there in the cloudera VM. There is a .jar file that controls the allocation of blocks of data to each individual data node (four in our case). 

### 6.2 Uploading the input file into HDFS:

We used the following steps to add our input file into the hadoop file system.

1) First navigate to the directory containing the file in the Terminal. <br>
2) type *hadoop fs -put purchases.txt myinput* into the terminal.
![Loading commands in the Terminal](https://github.com/Sid2110/PDC_project/blob/master/PaymentAnalysis/hadoop_load.JPG)

### 6.3 Development of Mapper and Reducer programs:

We have performed two types of analysis and for each we have defined the mapper and reducer programs in python. The codes have been uploaded in the Code section of our repository. <br>
During the allocation process, the mapper code is called to pick out the data required to be stored and analysed.

**Mapper (Written in Python):**
1. Each line, parsed from the big data file by the jar file, is read in by the mapper.
2.	Each individual field in the data file is read in.
3.	In our project, we are finding the details of the total sales, so the only entries needed are store location and cost of each sale.

**Reducer (Written in Python):**
1.	We have implemented the cluster with only one node. 
2.	Using the store location as the key and the sum of costs of sales as value, the data is processed and returned to the jar file for further processing and output.

### 6.4 Execution of the Mapper and Reducer programs:

Go to the directory containing your Mapper and reducer codes. Then, type the following command to run the MapReduce program:
hadoop jar /<location of your jar file/> -mapper mapper.py -reducer reducer.y -file mapper.py -file reducer.py -input myinput -output output1

myinput is the input directory containing your input file while output1 is the output directory where Hadoop will create and store the output file.

![Running the MapReduce program](https://github.com/Sid2110/PDC_project/blob/master/PaymentAnalysis/hadoop_exec.JPG)

Output File Generated:

![Output File generated by Hadoop](https://github.com/Sid2110/PDC_project/blob/master/PaymentAnalysis/hadoop_output.JPG)


Same procedure can be followed to do the second analysis.

### 6.5 Visualization of the output obtained from the analysis

The graphs have been developed from the output files we got from Hadoop. 

![Total Revnue Analysis](https://github.com/Sid2110/PDC_project/blob/master/TotalRevenueAnalysis/hadoop_revenue.JPG)

![Cash Analysis](https://github.com/Sid2110/PDC_project/blob/master/PaymentAnalysis/hadoop_cash.JPG)

![Credit Analysis](https://github.com/Sid2110/PDC_project/blob/master/PaymentAnalysis/hadoop_credit.JPG)

**The complete implementation of our project can be seen at the following YouTube link:** [https://youtu.be/qX8fcuilMW](https://youtu.be/qX8fcuilMW)
## 7. Result

From the first analysis we did, we can infer from the graphs which stores are making the most revenue in that time period and which are making the least. From the analysis, we can then distribute the resources like the inventory, advertising, etc to the stores that require them.

From the second analysis we did, we have seen which mode of payment is popular at any given store and see in which which stores cash mode is popular and in which stores, credit mode is popular. Thus based on the analysis, the company handling these stores, can introduce the appropriate supporting technology like the PIN machines etc. to the stores where credit cards are more popular.

The analysis can be made more accurate by introducing more features as well as more data, thereby making more precise and accurate analysis.